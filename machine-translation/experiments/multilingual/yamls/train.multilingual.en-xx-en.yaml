
## Where the samples will be written
save_data: opennmt/data/en-xx-en

# Vocabulary files, generated by onmt_build_vocab
src_vocab: opennmt/data/en-xx-en/vocab.src
tgt_vocab: opennmt/data/en-xx-en/vocab.tgt


# Vocabulary size - should be the same as in sentence piece
src_vocab_size: 32000
tgt_vocab_size: 32000

# # Tokenization options
src_subword_model: spm/vocab/multilingual.model
tgt_subword_model: spm/vocab/multilingual.model


src_seq_length: 64
tgt_seq_length: 64



# Training files
data:
    corpus_1:
        path_src: spm/train.spm.src
        path_tgt: spm/train.spm.tgt
        transforms: [filtertoolong]
    valid:
        path_src: spm/dev.spm.src
        path_tgt: spm/dev.spm.tgt
        transforms: [filtertoolong]





# General opts
log_file: opennmt/checkpoints/en-xx-en/log.txt
save_model: opennmt/checkpoints/en-xx-en/model_8E_8D
tensorboard_log_dir: opennmt/checkpoints/en-xx-en/tensorboard
tensorboard: true


average_decay: 0.0005
seed: 42
report_every: 10


# Total Sentences = 98790614
# sentence_in_batch = 64
# gradient_accumulation = 1
# total_gpus = 4
# effective_batch_size = sentence_in_batch * gradient_accumulation * total_gpus = 64 * 4 * 2 = 512
# total_steps = 98790614 / effective_batch_size = 98790614 / 512 ~ 193000
# evaluating 5 times in 1 epoch so evaltion steps = 193000 / 5 = 40000
# we will train for 10 epochs so steps = 193000 * 10 = 1930000
# 
# 

train_steps: 2000000



# grad accum of 4 and starts at step 0
# accum_count: [1]
# accum_steps: [0]




# validate every
valid_steps: 40000
# early stopping
early_stopping: 10 

keep_checkpoint: 10
save_checkpoint_steps: 40000


# # Number of GPUs, and IDs of GPUs
world_size: 4
gpu_ranks: [0,1,2,3]
master_port: 12345

# Batching
bucket_size: 262144
num_workers: 2  # Default: 2, set to 0 when RAM out of memory
batch_type: "tokens"

batch_size: 4096   
valid_batch_size: 2048

max_generator_batches: 2

# Optimization
model_dtype: "fp32"
optim: "adam"
learning_rate: 1
# Default: 4000 - for large datasets, try up to 8000

warmup_steps: 5000
decay_method: "noam"
adam_beta2: 0.98

# gradient clipping default is 5
max_grad_norm: 5

label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

# Model
encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 6
dec_layers: 6
heads: 8
hidden_size: 512
word_vec_size: 512
transformer_ff: 1024

dropout: [0.1]
dropout_steps: [0]

attention_dropout: [0.1]

# pos_ffn_activation_fn: "gelu"
